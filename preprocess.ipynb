{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:  2260668 \t Columns:  145\n"
     ]
    }
   ],
   "source": [
    "full_data = pd.read_csv(\"loan.csv\", low_memory=False)\n",
    "\n",
    "print(\"Rows: \", full_data.shape[0], \"\\t Columns: \", full_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains records of ~2.2M clients who borrowed money on the platform between 2007-2018. Information  is stored in 145 columns, some of which are filled only once (e.g. at the time of application or only in the case of a particular event occuring), some updated periodically, with only most recent value present. As the aim of the model is to predict default before the loan is issued, we need to make sure that there are is no data leakage, i.e. that training data consists only of information that is known at the time of application. Unfortunately, in case of many columns there exists ambiguity about the point of time it refers to. Since there is no safe way of resolving it other than reaching to the source, we do not risk compromising the integrity of our models and simply will not include these columns in the training data (more in _columns.xlsm_ file ).         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"addr_state\",\n",
    "    \"annual_inc\",\n",
    "    \"annual_inc_joint\",\n",
    "    \"application_type\",\n",
    "    \"disbursement_method\",\n",
    "    \"earliest_cr_line\",\n",
    "    \"emp_length\",\n",
    "    \"home_ownership\",\n",
    "    \"initial_list_status\",\n",
    "    \"installment\",\n",
    "    \"int_rate\",\n",
    "    \"issue_d\",\n",
    "    \"loan_status\",\n",
    "    \"pub_rec_bankruptcies\",\n",
    "    \"purpose\",\n",
    "    \"sec_app_earliest_cr_line\",\n",
    "    \"sub_grade\",\n",
    "    \"term\",\n",
    "    \"verification_status\",\n",
    "    \"verification_status_joint\",\n",
    "]\n",
    "\n",
    "data = full_data.loc[:, cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defaulted loans constitue 12.5% of the whole data set, furthermore, about 40% of the data points correspond to current loans, which are neither default nor fully paid, as we can infer from the base rates, some of them will default in the future, but as we do not know which ones, they are irrelevant for the purpose of building a model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fully Paid                                             1041952\n",
       "Current                                                 919695\n",
       "Charged Off                                             261655\n",
       "Late (31-120 days)                                       21897\n",
       "In Grace Period                                           8952\n",
       "Late (16-30 days)                                         3737\n",
       "Does not meet the credit policy. Status:Fully Paid        1988\n",
       "Does not meet the credit policy. Status:Charged Off        761\n",
       "Default                                                     31\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.loan_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label defaulted loans as 1 and fully paid loans as 0.\n",
    "\n",
    "def_types = [\n",
    "    \"Charged Off\",\n",
    "    \"Late (31-120 days)\",\n",
    "    \"Does not meet the credit policy. Status:Charged Off\",\n",
    "    \"Default\",\n",
    "]\n",
    "def_mask = (\n",
    "    (data[\"loan_status\"] == def_types[0])\n",
    "    | (data[\"loan_status\"] == def_types[1])\n",
    "    | (data[\"loan_status\"] == def_types[2])\n",
    "    | (data[\"loan_status\"] == def_types[3])\n",
    ")\n",
    "paid_mask = data[\"loan_status\"] == \"Fully Paid\"\n",
    "\n",
    "d0 = data[paid_mask]\n",
    "d0[\"loan_status\"].values[:] = 0\n",
    "d1 = data[def_mask]\n",
    "d1[\"loan_status\"].values[:] = 1\n",
    "\n",
    "data = pd.concat([d1, d0], ignore_index=True)\n",
    "data.loan_status = pd.to_numeric(data.loan_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns _issue_d, (sec_app_)earliest_cr_line_ contain dates. As the data was collected over an extended period of time, values in _(sec_app_)earliest_cr_line_ are not directly comparable. Moreover, the end goal of our model is to work with unseen data, hence _issue_d_ must also be sensibly transformed. One way of achieving both is to create new features, namely, _(_sec_app_)yrs_since_last_cr_line_ and _issue_quarter,_ the first two are now directly comparable, whereas the third enables us to take into account seasonal patterns in the data (e.g. borrowers in the holiday season may be more likely to act on impulse, hence more likely to default etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features and discard old ones\n",
    "data.issue_d = pd.to_datetime(data.issue_d)\n",
    "data.earliest_cr_line = pd.to_datetime(data.earliest_cr_line)\n",
    "\n",
    "data.sec_app_earliest_cr_line = pd.to_datetime(data.sec_app_earliest_cr_line)\n",
    "data[\"sec_app_yrs_since_last_cr_line\"] = (\n",
    "    data.issue_d - data.sec_app_earliest_cr_line\n",
    ") / np.timedelta64(1, \"Y\")\n",
    "\n",
    "data[\"yrs_since_last_cr_line\"] = (\n",
    "    data.issue_d - data.earliest_cr_line\n",
    ") / np.timedelta64(1, \"Y\")\n",
    "data[\"issue_quarter\"] = data.issue_d.dt.quarter\n",
    "\n",
    "data = data.drop(\n",
    "    columns=[\"issue_d\", \"earliest_cr_line\", \"sec_app_earliest_cr_line\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to deal with non-numerical columns. In our data set there are two types of such data we need to treat separately. For _sub_grade_ and _emp_length_ there exists a natural ordering that we want to preserve, whereas for categories such as _purpose_ this does not hold. For the former we map them to natural numbers according to their order, while for the latter we will use one-hot encoding i.e. each category will become an independent feature taking values in $\\{0,1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode ordinal categories\n",
    "vs_sub_grade = data.sub_grade.unique()\n",
    "vs_sub_grade.sort()\n",
    "vs_sub_grade = {vs_sub_grade[i]: i for i in range(len(vs_sub_grade))}\n",
    "data.sub_grade = data.sub_grade.replace(vs_sub_grade)\n",
    "\n",
    "vs_emp_length = {\n",
    "    \"2 years\": 2,\n",
    "    \"3 years\": 3,\n",
    "    \"10+ years\": 10,\n",
    "    \"8 years\": 8,\n",
    "    \"5 years\": 5,\n",
    "    \"6 years\": 6,\n",
    "    \"< 1 year\": 0,\n",
    "    \"4 years\": 4,\n",
    "    \"7 years\": 7,\n",
    "    np.nan: 0,\n",
    "    \"9 years\": 9,\n",
    "    \"1 year\": 1,\n",
    "}\n",
    "data.emp_length = data.emp_length.replace(vs_emp_length)\n",
    "\n",
    "vs_term = {\" 36 months\": 3, \" 60 months\": 5}\n",
    "data.term = data.term.replace(vs_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid adding 50 extra features we group address states into 4 regions as defined by the US Census Bureau. This transformation should be able to preserve information contained in the applicant's location, without drastically increasing the number of features and possibly reducing bias stemming from small amount of data in some locations (e.g. there were only 14 customers from Indiana)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_regions = {\n",
    "    \"AK\": \"West\",\n",
    "    \"AL\": \"South\",\n",
    "    \"AR\": \"South\",\n",
    "    \"AZ\": \"West\",\n",
    "    \"CA\": \"West\",\n",
    "    \"CO\": \"West\",\n",
    "    \"CT\": \"Northeast\",\n",
    "    \"DC\": \"South\",\n",
    "    \"DE\": \"South\",\n",
    "    \"FL\": \"South\",\n",
    "    \"GA\": \"South\",\n",
    "    \"HI\": \"West\",\n",
    "    \"IA\": \"Midwest\",\n",
    "    \"ID\": \"West\",\n",
    "    \"IL\": \"Midwest\",\n",
    "    \"IN\": \"Midwest\",\n",
    "    \"KS\": \"Midwest\",\n",
    "    \"KY\": \"South\",\n",
    "    \"LA\": \"South\",\n",
    "    \"MA\": \"Northeast\",\n",
    "    \"MD\": \"South\",\n",
    "    \"ME\": \"Northeast\",\n",
    "    \"MI\": \"Midwest\",\n",
    "    \"MN\": \"Midwest\",\n",
    "    \"MO\": \"Midwest\",\n",
    "    \"MS\": \"South\",\n",
    "    \"MT\": \"West\",\n",
    "    \"NC\": \"South\",\n",
    "    \"ND\": \"Midwest\",\n",
    "    \"NE\": \"Midwest\",\n",
    "    \"NH\": \"Northeast\",\n",
    "    \"NJ\": \"Northeast\",\n",
    "    \"NM\": \"West\",\n",
    "    \"NV\": \"West\",\n",
    "    \"NY\": \"Northeast\",\n",
    "    \"OH\": \"Midwest\",\n",
    "    \"OK\": \"South\",\n",
    "    \"OR\": \"West\",\n",
    "    \"PA\": \"Northeast\",\n",
    "    \"RI\": \"Northeast\",\n",
    "    \"SC\": \"South\",\n",
    "    \"SD\": \"Midwest\",\n",
    "    \"TN\": \"South\",\n",
    "    \"TX\": \"South\",\n",
    "    \"UT\": \"West\",\n",
    "    \"VA\": \"South\",\n",
    "    \"VT\": \"Northeast\",\n",
    "    \"WA\": \"West\",\n",
    "    \"WI\": \"Midwest\",\n",
    "    \"WV\": \"South\",\n",
    "    \"WY\": \"West\",\n",
    "}\n",
    "data.addr_state = data.addr_state.replace(us_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns have missing values. For _annual_inc_ we fill them with median. Columns related to secondary applicant have missing values only if there is no secondary applicant. Since we do not want them to contribute to the input in the case of an individual application, we fill them with zeros if they are numerical and do not one-hot encode them if they are categorical. For _earliest_cr_line_ and _emp_length_ we make an assumption that if there is no information provided we assign lowest value from the range, as typically being employed for a long time or having a long credit history plays in favour of the applicant, so withholding such information is more probable in the case of the opposite. For _pub_rec_bankruptcies_ and no value probably means that the applicants record is clear. Additionally we will use secondary applicant income as a feature instead of joint income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annual_inc_joint\n",
      "pub_rec_bankruptcies\n",
      "sec_app_yrs_since_last_cr_line\n",
      "verification_status_joint\n",
      "yrs_since_last_cr_line\n"
     ]
    }
   ],
   "source": [
    "for a in sorted(data.columns[data.isna().any()]):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing income with median income\n",
    "data[\"annual_inc\"].fillna(data[\"annual_inc\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate secondary applicant income\n",
    "data[\"sec_app_annual_inc\"] = data[\"annual_inc_joint\"] - data[\"annual_inc\"]\n",
    "data = data.drop(columns=\"annual_inc_joint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addr_state\n",
      "application_type\n",
      "disbursement_method\n",
      "home_ownership\n",
      "initial_list_status\n",
      "issue_quarter\n",
      "purpose\n",
      "verification_status\n",
      "verification_status_joint\n"
     ]
    }
   ],
   "source": [
    "# get a list of categorical columns\n",
    "cat_cols = [\"issue_quarter\"]\n",
    "for i in range(len(data.dtypes)):\n",
    "    if data.dtypes[i] == \"object\":\n",
    "        cat_cols.append(data.columns[i])\n",
    "for name in sorted(cat_cols):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode categorical variables and fill remaining with zeros\n",
    "data = pd.get_dummies(data, prefix=cat_cols, columns=cat_cols)\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "data.to_csv(\"preprocessed2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
